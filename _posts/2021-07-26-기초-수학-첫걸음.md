---
layout: post
title: 📝 Pre-course | 기초 수학 첫걸음
date: 2021-07-26 13:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 머신러닝 수학의 기초가 되는 개념들을 훑어보는 시간
tag:
  - Boostcamp AI
  - Pre-course
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. 벡터(Vector)
### 정의

* 벡터란? 숫자를 원소로 가지는 **리스트**(list) 또는 **배열**(array)

```py
x = [1, 7, 2]
x = np.array([1, 7, 2])
```

* 원점으로부터 **상대적 위치**를 표현
* 두 벡터의 덧셈(뺄셈)은 다른 벡터로부터 상대적 위치이동을 표현

### 노름

* 벡터의 노름(norm): 원점에서부터의 거리
  * L1: 각 성분의 **변화량의 절대값**을 모두 더함
  * L2: 피타고라스 정리를 이용해 **유클리드 거리**를 계산 

$$\left\|x\right\|_{1}=\sum_{i=1}^{d}\lvert x_{i} \rvert$$

$$\left\|x\right\|_{2}=\sqrt{\sum_{i=1}^{d}\lvert x_{i} \rvert^{2}}$$

* 두개의 다른 노름
  * **기하학적 성질**의 차이
  * 거리의 정의에 따라서 달라짐
  * L1: Robust 학습, Lasso 회귀
  * L2: Laplace 근사, Ridge 회귀

<figure>
<img src="/assets/img/IMG_1175.jpg" alt="L1과 L2 노름 상의 원">
<figcaption>Fig 1. L1과 L2 노름 상의 원</figcaption>
</figure>

### 벡터 간의 거리

* 두 벡터 사이의 거리를 계산할 때는 **벡터의 뺄셈**을 이용

$$\left\| y - x \right\| = \left\| x - y \right\|$$

### 벡터 사이의 각도
🎈 L2 노름에서만 가능
* **제2 코사인 법칙**으로 계산

$$\cos\theta = \frac{\left\| x \right\|_{2}^2 + \left\| y \right\|_{2}^2 - \left\| x - y \right\|_{2}^2}{2\left\| x \right\|_{2}\left\| y \right\|_{2}}$$

* 분자를 **내적(inner product)**으로 표현 가능함

$$<x,y> = \sum_{i=1}^{d}x_{i}y_{i}$$

* 내적은 `np.inner`를 이용해서 계산

```py
def angle(x,y):
  v = np.inner(x,y) / (l2_norm(x) * l2_norm(y))
  theta = np.arccos(v)
  return theta
```

##### 내적은 어떻게 해석할까?
- 정사영(orthogonal projection)된 벡터의 길이와 관련 있음
- Proj($$\textbf{x}$$)의 길이는 코사인법칙에 의해 $$\left\|\textbf{x}\right\|\cos\theta$$가 됨
- 내적은 **정사영의 길이**를 **벡터 $$y$$**의 길이 **$$\left\|\textbf{y}\right\|$$**만큼 조정한 값

<figure>
<img src="/assets/img/IMG_1176.jpg" alt="정사영된 벡터와 내적의 정의" width="500">
<figcaption>Fig 2. 정사영된 벡터와 내적</figcaption>
</figure>

🎈 두 벡터의 유사도(similarity)를 측정하는데 사용 가능

# 2. 행렬(Matrix)
### 정의

* 벡터를 원소로 가지는 **2차원 배열**

~~~py
X = np.array([[1, -2, 3],
              [7, 5, 0],
              [-2, -1, 2]])
~~~

* 벡터가 공간에서 한 점을 의미한다면 행렬은 여러 점들을 나타냄
* 행렬의 행벡터 $$\textbf{x}_{i}$$는 **i번째 데이터**를 의미
* 행렬의 $$x_{ij}$$는 **i번째 데이터의 j번째 변수의 값**을 말함

<figure>
<img src="/assets/img/IMG_1177.jpg" alt="행렬">
<figcaption>Fig 3. 행이 n개, 열이 m개인 행렬</figcaption>
</figure>

<figure>
<img src="/assets/img/IMG_1178.jpg" alt="2차원 공간의 행렬" width="450">
<figcaption>Fig 4. 2차원 공간으로 표현한 행렬</figcaption>
</figure>

### 행렬 곱셈

* **i번째 행벡터**와 **j번째 열벡터** 사이의 내적을 성분으로 가지는 행렬을 계산
* `numpy`에선 `@` 연산을 사용

```py
X @ Y
```

##### 행렬의 내적?

* `np.inner`는 **i번째 행벡터**와 **j번째 행벡터** 사이의 내적을 성분으로 가지는 행렬을 계산

```py
np.inner(X, Y)
```

🎈 수학에서 말하는 내적과는 다름

* 벡터공간에서 사용되는 연산자(operator)로 사용됨
* 벡터를 **다른 차원의 공간**으로 보낼 수 있음
* 이걸로 패턴을 추출할 수 있고 데이터를 압축할 수 있음

<figure>
<img src="/assets/img/IMG_1179.jpg" alt="행렬곱의 역할">
<figcaption>Fig 4. 행렬곱으로 m차원 공간의 행렬 x에서 n차원 공간의 행렬 z로 변형하는 행렬 A</figcaption>
</figure>

### 역행렬

* 어떤 행렬 A의 연산을 거꾸로 되돌리는 행렬을 **역행렬(inverse matrix)**이라 부름
  * 행과 열 숫자가 같고 (n=m)
  * 행렬식(determinant)이 0이 아니어야함

$$AA^{-1}=A^{-1}A=I$$

* 만일 계산할 수 없다면...
  * 유사역행렬(pseudo-inverse) 또는 무어펜로즈(Moore-Penrose) 역행렬

$$A^+ = (A^{\top}A)^{-1}A^{\top};\quad A^{+}A=I;\quad n \geq m$$

$$A^+ = A^{\top}(A^{\top}A)^{-1};\quad AA^{+}=I;\quad n \leq m$$

🎈 $$n\geq m$$: 데이터가 변수 개수보다 많거나 같음, $$n\leq m$$: 식이 변수 개수보다 작거나 같음

* `numpy.linalg.inv`로 역행렬을,
* `numpy.linalg.pinv`로 무어펜로즈 역행렬을 구할 수 있음

~~~py
np.linalg.inv(X)
np.linalg.pinv(Y)
~~~

### 응용1: 연립방정식

* `np.linalg.pinv`를 이용하여 연립방정식의 해를 구할 수 있음 ($$n\leq m$$)

$$a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1m}x_{m}=b_{1}\\a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2m}x_{m}=b_{2}\\\vdots\\a_{n1}x_{1}+a_{n2}x_{2}+\cdots+a_{nm}x_{m}=b_{n}$$

$$\Rightarrow\textbf{A}\textbf{x}=\textbf{b}\\\Rightarrow \textbf{x} = \textbf{A}^+\textbf{b} = A^{\top}(A^{\top}A)^{-1}\textbf{b}$$

### 응용2: 선형회귀분석

* 데이터를 선형모델(linear model)로 해석하는 선형회귀식을 찾을 수 있음

<figure>
<img src="/assets/img/IMG_1180.jpg" alt="선형회귀분석">
<figcaption>Fig 5. 무어펜로즈를 이용한 선형회귀분석</figcaption>
</figure>

```py
# Scikit Learn을 활용한 회귀분석
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
y_test = model.predict(x_test)

# Moore_Penrose 역행렬
X_ = np.array([np.append(x,[1]) for x in X]) # y-intercept 항 추가
beta = np.linalg.pinv(X_) @ y
y_test = np.append(x, [1]) @ beta
```

# 3. 경사하강법(순한맛)

### 미분

* 변수의 움직임에 따른 함수값의 변화를 측저하기 위한 도구
* 최적화에서 제일 많이 사용하는 기법

$$f'(x)=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}$$

* `sympy.diff`로 계산 가능

```py
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3), x)
```

* 미분은 함수 $$f$$의 주어진 점 $$(x,f(x))$$에서의 접선의 기울기를 구함
* 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있음

<figure>
<img src="/assets/img/IMG_1181.jpg" alt="접선의 기울기">
<figcaption>Fig 6. (x,f(x))에서의 접선의 기울기</figcaption>
</figure>

* 미분값을 더하면 경사상승법(gradient ascent), 미분값을 빼면 경사하강법(gradient descent)
* 각 함수의 극대값과 극소값의 위치를 구할 때 사용함

### 경사하강법: 알고리즘

```py
# gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while abs(grad) > eps:
  var = var - lr * grad
  grad = gradient(var)
```

🎈 컴퓨터로 미분이 정확히 0이 되는 것은 불가능하므로 `eps`보다 작을 때 종료하는 조건이 필요

🎈 `lr`은 학습률로서 미분을 통해 업데이트하는 속도를 조절함

### 변수가 벡터이면?

* 다변수 함수인 경우 편미분(partial differentiation)을 사용

$$\partial_{x_{i}}f(\textbf{x})=\lim_{h\rightarrow0}\frac{f(\textbf{x}+h\textbf{e}_{i})-f(\textbf{x})}{h}$$

* $$\textbf{e}_{i}$$는 $$i$$번째 값만 1이고 나머지는 0인 단위벡터
* 그레디언트(gradient) 벡터를 이용하여 경사하강/경사상승법에 적용

$$\nabla f=(\partial_{x_{1}}f,\partial_{x_{2}}f,\cdots,\partial_{x_{d}}f)$$

```py
import sympy as sym
from sympy.abc import x,y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)
# >> 2*x + 2*y - sin(x + 2*y)
```

<figure>
<img src="/assets/img/IMG_1182.jpg" alt="그레디언트 벡터">
<figcaption>Fig 7. 그레디언트 벡터</figcaption>
</figure>