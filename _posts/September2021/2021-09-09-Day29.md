---
layout: post
title: 📔 Daily Report | Day 29
date: 2021-09-09 10:15 +0900
#modified: 2021-07-30 18:49 +0900
description: Day 29
tag:
  - Boostcamp AI
  - Daily Report
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. 수업 노트


# 2. 피어 세션

💡 **공유 사항**

Q. 어제 질문 중 hard prediction/soft prediction 관련 
(다른 ppt에도 soft prediction이 있어서 오타가 아닌가? 하고 질문했습니다.  김종하 멘토님의 답변 공유합니다. (이윤영))

A. 여기서 T=1인 softmax의 output도 soft prediction이라고 표시한 이유는, 실제 정답과 같이 완전 one-hot([0,0,1,0])으로 나오는 것이 아니라 model output이 [0, 0.1, 0.9, 0]과 같은 형태로 나오기 때문에 soft하다고 표시한 것입니다!
soft prediction : model output (e.x [0, 0.1, 0.9, 0])
hard prediction : final output (e.x argmax([0, 0.1, 0.9, 0] = 2 (혹은 one-hot으로 나타내면 [0, 0, 1, 0]))본 강의에서는 이런 의미의 notation이라고 생각해주시면 될 것 같습니다
smoothing이 없더라도 discrete한 예측 값이 아니라 확률값으로 나타나는 값이기 때문에 soft prediction이라고 표현했는데, 아무래도 다른 글에서 사용하는 notation이랑 헷갈리실 수 있을 것 같네요. 
논문 링크입니다 : [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)
→ 멘토님께 추가 질문 하기로 함. 

💡 **수업 질문 (3강)**

Q. 1x1 convolution가 무엇을 하는 것일까?
1x1 convolution을 하면 channel수를 바꿔줌. 

Q. 1x1 convolution 구현 방식?

```python
#1x1 convolution in pytorch (출처 : https://coding-yoon.tistory.com/116)
nn.Conv2d(in_channels=in_dim, out_channels=mid_dim, kernel_size=1, bias=False)
```

💡 **과제 질문**

Q. 선택 과제3에서 VGG11 Segmentation initialization할 때  torch. no_grad()?

 질문 게시판에 질문 올림. 추후 공유

💡 **TMI 세션 : 이것만 알면 나도 영어 고수!! (채윤 캠퍼님)**

발음! 발음! 발음!

→ 발음 꿀팁1 : R과 L을 구분하기 

→ 발음 꿀팁2 : J와 Z를 구분하기 - J=줴 Z=즤 : jacket : 줴킷 amazon : 아마즈언

→ 구글 활용하기 : 단어 + meaning → 발음 듣기! 영화 · 드라마 · 유튜브 활용!

# 3. 오늘 마치며...

오늘은 segmentation에 대해서 배웠는데 어떤 식으로 classification과 다른지 어떻게 동작하는지 무슨 논문이 있는지에 대해서 알아봤는데 사람들은 굉장히 많은 방법으로 문제를 해결하려고 노력하는 구나라는 생각이 들었다. 특히 논문들을 보면 성능을 높이기 위해서 포기할 것과 챙겨할 것 그 경계선에서 leverage하는 그 방법들이 참 신기하다. 이것이 노력의 산물이겠지? 많이 배우고 나도 빨리 써먹도록 하자.