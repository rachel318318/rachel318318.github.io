---
layout: post
title: 🔍 Object Detect Comp | 2 Stage Detectors
date: 2021-09-28 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 객체 검출 대회를 위한 모델을 만드는 과정
tag:
  - Boostcamp AI
  - P-stage
  - Object Detection Competition
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 2 Stage Detectors

R-CNN, SPPnet, Fast R-CNN, Faster R-CNN

## Background

사람 → 입력 이미지 → 생각,,, → 위치보기(배경과 다른 무언가) → 물체이름 맞추기

기계 인공지능 등 사람과 비슷하게 생각함

컴퓨터 → 입력 이미지 → 계산 중,,, → localization(객체가 있을 법한 위치 추정) → classification(객체 식별)

앞으로 배울 객체가 있을 법한 곳을 특정 짓고 해당 객체가 무엇인지에 대해서 식별 → 이 두 단계를 거치는 모델이 2 stage detectors

## R-CNN

Deep-learning을 이용해서 객체 검출을 하려고 한 최초의 페이퍼

초기 모델인 만틈 직관적이다

입력 이미지 → 객체가 있을법한 후보 영역을 예측 (sliding window, selective search) → 후보영역을 리사이즈 → CNN → sementic feature를 뽑음 → Classify(객체야! 뭐야! 아니야 배경이야!)

Sliding window

사용할 수 있는 window의 모습은 다양하다

윈도우를 통해 이미지를 통과시킨다

하지만 이렇게 뽑으면 무수히 많은 후보영역이 나올 것 → 비효율적

Selective Search

이미지의 색깔, 질감, shape 이미지에 존재하는 특성들을 활용, 작은 영역으로 나누고 통합해간다

초기에는 많은 후보영역들이 있지만 점차 통합을 해나가면서 segmentation들을 줄여나감

Pipeline

입력 이미지 받기 → selective search를 통해 약 2000개의 RoI(후보영역) 추출 → 추출된 후보영역의 크기를 조절해 모두 동일한 사이즈로 변형(warping) (CNN의 마지막인 FC layer의 입력 사이즈가 고정이기 때문에) → RoI를 CNN에 넣어 feature 추출 → CNN을 통해 나온 2000개의 feature(sementic 정보 포함)를 SVM에 넣어 분류 진행 → CNN을 통해 나온 feature를 regression을 통해 bbox를 예측 (후보위치들을 더욱 정확한 위치로 변형시켜주는 것) bbox의 중심점을 ground truth의 중심점으로 옯기고 width와 height까지 바꾸는 델타를 학습

Training

AlexNet

- Domain specific finetuning
- Dataset
    - IoU > 0.5
    - IoU < 0.5
    - Positive samples 32, negative sampels 96
- Heard negative mining
    - Hard negative: False positive
    - 배경으로 식별하기 어려운 샘플들을 강제로 다음 배치의 negative sample로 mining
    - 모델이 구분하기 어려운 샘플들을 다음 배치에 추가시켜서 학습시키는 것
- Bbox regressor
    - Dataset 구성
        - IoU > 0.6: positive
- Loss
    - MSE Loss

Shortcomings

- 2000개의 region을 각각 CNN 통과
- 다양한 객체 사이즈를 강제 warping (정보 손실) 성능 하락 가능성
- CNN, SVM, bbox regressor 따로 학습
- End-to-end X 구조적인 측면에서 한계점을 가진

## SPPNet

Overview

image → crop/warp → conv layers → fc layers → output

image → conv layers → spatial pyramid pooling → fc layers → output

Conv neural net 수행을 feature extraction을 한번만 수행하면 된다

Spatial Pyramid Pooling

뽑아놓은 feature map에서 2000개의 region을 뽑음 warping하지 않고 spatial pyramid pooling을 통해서 고정된 사이즈로 변환

어떠한 RoI projection을 통해서 2000개의 region을 뽑는다 다양한 사이즈 고정된 feature vector로 변환하기 위해서 spp

다양한 roi로 부터 target feature size를 정한다

## Fast R-CNN

spatial pyramid pooling → RoI pooling

고정된 사이즈의 feature vector

- VGG16 사용
- RoI Project을 통해 feature map 상에서 RoI 계산
    - selective search를 적용하는 것은 불가능
    - selective search 2000개의 region, RoI들을 뽑아옴
    - 그 region들을 feature map상에 투영시킴
    - CNN을 통과해서 원본 이미지 보다 작아졌을때, 비율을 맞춰서 region을 축소시켜야함
- RoI pooling을 통해서 고정된 사이즈의 feature 추출
- fully connected layer 이후, softmax classifier, bounding bbox
- multi task loss 사용 (classification loss, bbox regression)
    - class: Cross entropy
    - bbox: Smooth L1
- Dataset 구성
    - IoU > 0.5
    - 0.1 < IoU < 0.5
- Hierarchical sampling
- selective search → CPU 상에서 돌아감, 학습가능한 알고리즘 x → 진정한 end-to-end가 아님

## Faster R-CNN

RPN → Region Proposal도 학습 가능하게 바꿔줌

conv feature map에서 어떻게 region을 뽑는가?

n개의 Anchor box 정의 → 객체 사이즈에 따라서 대응 가능

feature map 각 셀마다 k개의 anchor box가 있다

RPN이 그 anchor box에 객체가 포함되어 있는지 아닌지 판단한다

있다면 resite하고 크기를 조정한다

NMS

겹치는 영역이 매우 많을 수도 있다

IoU가 0.7이상이면 중복됐을 가능성이 높아 제거한다 → class score를 0으로 바꿔줌