---
layout: post
title: âœï¸ U-stage | Optimization
date: 2021-08-10 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: ë¨¸ì‹ ëŸ¬ë‹ ìˆ˜í•™ì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ê°œë…ë“¤ì„ í›‘ì–´ë³´ëŠ” ì‹œê°„
tag:
  - Boostcamp AI
  - U-stage
  - DL Basics
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. Important Concepts

### Generalization

* How well the learned model will behave on unseen data
* We would like to minimize the generalization gap

<figure>
<img src="/assets/img/IMG_1236.jpg" alt="generalization">
<figcaption>Fig 1. Generalization</figcaption>
</figure>

### Underfitting/Overfitting

<figure>
<img src="/assets/img/IMG_1237.jpg" alt="under overfitting" width="550">
<figcaption>Fig 2. Underfitting and Overfitting</figcaption>
</figure>

### Cross-validation

* A model validation technique for assessing how the model will generalize to an independent (test) data set
* Divides train data into multiple folds
* Validation data are used to tune hyperparameters
    * Hyperparameters are the parameters that you determine
* After the hyperparameters are set, the model is good to learn the whole data

<figure>
<img src="/assets/img/IMG_1238.jpg" alt="cross validation" width="450">
<figcaption>Fig 3. Cross-validation</figcaption>
</figure>

ğŸˆ Test data must not be used in any cases for training

### Bias and Variance

* Variance = Precision
* Bias = Accuracy

##### Bias and Variance Tradeoff

* Minimizing (cost) can be decomposed into three different parts
    * Bias^2
    * Variance
    * Noise

<figure>
<img src="/assets/img/IMG_1241.jpg" alt="bias variance">
<figcaption>Fig 4. Bias and Variance Tradeoff</figcaption>
</figure>

* When we decrease variance, it causes bias to increase and vice versa

### Bootstrapping

* Any test or metric that uses random sampling with replacement

##### Bagging (Bootstrapping aggregating)
* Multiple models are being trained with bootstrapping (simultaneously)
* Individual predictions are aggregated at the end (averaging, etc.)

##### Boosting
* For specific training samples that are hard to classify
* A strong model is built by combining weak models (a model becomes stronger and stronger in a sequence)

<figure>
<img src="/assets/img/IMG_1239.jpg" alt="bagging and boosting" width="550">
<figcaption>Fig 5. Bagging and Boosting</figcaption>
</figure>

# 2. Gradient Descent Methods

### Methods by Batch Size

* Stochastic gradient descent
    * Update with the gradient computed from a single sample
* Mini-batch gradient descent
    * " from a subset of data
* Batch gradient descent
    * " from the whole data

### Batch Size Matters

* There is a difference in generalization performance between small-batch methods and large-batch methods
* Small-batch methods consitently converge to flat minimizers
* As shown in the figure, the sharp minimum results in a big error from the testing function compared to the flat minimum

<figure>
<img src="/assets/img/IMG_1240.jpg" alt="sharp and flat minimum" width="600">
<figcaption>Fig 6. Small and Large Batch Performance</figcaption>
</figure>

### Practical GD Methods

##### gradient descent

<figure>
<img src="/assets/img/IMG_1242.jpg" alt="gd" width="400">
</figure>

##### Momentum
* Takes in previous gradients and the way they were heading towards
* Faster to reach the minimum than SGD

<figure>
<img src="/assets/img/IMG_1243.jpg" alt="momentum" width="450">
</figure>

##### Nesterov accelerated gradient
* Adjusted momentum approach
* Might be faster to reach the minimum than momentum

<figure>
<img src="/assets/img/IMG_1244.jpg" alt="nesterov" width="400">
</figure>

##### Adagrad
* Adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters
* As $$G_t$$ increases, the learning rate decreases

<figure>
<img src="/assets/img/IMG_1245.jpg" alt="adagrad" width="500">
</figure>

##### Adadelta
* Reduce its monotonically decreasing the learning rate by restricting the accumulation window
* No learning rate used

<figure>
<img src="/assets/img/IMG_1246.jpg" alt="adadelta" width="550">
</figure>

##### RMSprop
* Proposed by Geoff Hinton in his lecture

<figure>
<img src="/assets/img/IMG_1247.jpg" alt="RMSprop" width="550">
</figure>

##### Adam
* Effectively combines momentum with adaptive learning rate

<figure>
<img src="/assets/img/IMG_1248.jpg" alt="Adam" width="550">
</figure>

# 3. Regularization

* Hinders the learning process to challenge a learning model
* Tools to achieve generalization

### Early Stopping

* As a training error decreasses, there is a high chance that a test error increases
* In early stopping, validation error is used to impede the training at a certain gap
* Additional validation data are needed

<figure>
<img src="/assets/img/IMG_1249.jpg" alt="early stopping">
<figcaption>Fig 7. Early Stopping</figcaption>
</figure>

### Parameter Norm Penalty

* Assumes that generalization works better when the function space is smooth
* Adds smoothness to the function space by decreasing parameters

<figure>
<img src="/assets/img/IMG_1252.jpg" alt="parameter norm" width="500">
<figcaption>Fig 8. Parameter Norm Penalty</figcaption>
</figure>

### Data Augmentation

* The more data the better
* By data augmentation, we give data a variance (deforming pictures, etc.)
    * ex. label preserving augmentation

### Noise Robustness

* Add random noises inputs or weights

<figure>
<img src="/assets/img/IMG_1253.jpg" alt="noise robustness" width="550">
<figcaption>Fig 9. Noise Robustness</figcaption>
</figure>

### Label Smoothing

* Mix-up constructs augmented training examples by mixing both input and output of two randomly selected training data
* CutMix contructs austmented training examples by mixing inputs with cut and paste and outputs with soft labels of two randomly selected training data

<figure>
<img src="/assets/img/IMG_1250.jpg" alt="Label Smoothing">
<figcaption>Fig 9. Label Smoothing</figcaption>
</figure>

### Dropout

* In each forward pass, randomly sets some neurons to zero and gives more robust functionality

### Batch Normalization

* Computes the empirical mean and variance independently for each dimension (layers) and normalize

