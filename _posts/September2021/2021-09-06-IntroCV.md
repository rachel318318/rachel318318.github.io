---
layout: post
title: ✏️ U-stage CV | Intro to CV & Image Classification
date: 2021-09-06 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: Computer Vision에 대한 심화 학습
tag:
  - Boostcamp AI
  - U-stage Computer Vision
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# Conputer Vision

### Couse Overview

##### Artificial Intelligence
* 먼저 AI에 대해서 알아보자
    * 사고를 통해 인과관계를 파악하는 것, 소리, 지각능력, understanding 모두 지능에 포함
    * 인공지능의 reference는 사람이므로 사람을 연구해보자
* 유아기 시절에 우리는 세상과 interaction을 하는 방법을 배운다
    * 오감을 활용해서 지각능력을 발달시킨다
    * 지각능력으로 인과관계를 이해하고 사고능력을 키운다
* 위 논리를 기계에 적용시켜보자
    * 지각능력이 제공하는 것 -> (input과 output) data
    * 오감에서 더 나아가 표정, 말투, 신체적 상호작용, 사회적 기준 등 복합적인 감각을 이용하여 input과 output data를 제공한다
    * 아무튼 기계로서 **지각능력 확립이 필수**이다
* 시각이 가장 많은 정보를 처리한다고 한다 (75% of incoming information)

##### Computer Vision

<figure>
<img src="/assets/img/Screen Shot 2021-09-06 at 9.00.40 PM.png" alt="human vision steps">
<figcaption>Fig 1. 우리가 사물을 보는 과정</figcaption>
</figure>

<figure>
<img src="/assets/img/Screen Shot 2021-09-06 at 9.00.48 PM.png" alt="computer vision steps">
<figcaption>Fig 2. 컴퓨터가 사물을 보는 과정</figcaption>
</figure>

- 컴퓨터 비전 지각능력의 종류는 다양하다
    - Color perception
    - Motion perception
    - 3D perception
    - Semantic-level perception
    - Social perception (emotion perception)
    - Visuomotor perception, etc.
- 그러나 컴퓨터 비전의 핵심은 **사람의 시각 능력을 이해하고 어떤식으로 구현할지 연구하는 것**
- 사람의 시각 자체가 사실상 **불완전**하다
    - 아래의 사진을 예로 우리는 똑바로 서있는 사람만 봐왔기에 시각 기능이 biased되어 있다는 것을 알 수 있다
    - 이러한 불완전성을 보완하는 것이 컴퓨터 비전의 또다른 핵심

<figure>
<img src="/assets/img/Screen Shot 2021-09-06 at 9.21.05 PM.png" alt="human vision imperfection">
<figcaption>Fig 3. Optical Illusion</figcaption>
</figure>

- Machine Learning에서의 Computer Vision
    - 전문가가 직접 데이터 추출, 특징화에 개입해서 방법론을 설계했다
    - 간단한 모델 활용
- 그에 반해 Deep Learning에서는 gradient descent 등과 같은 방법으로 **직접 기계가 특징 추출법과 판별법을 사용**한다
    - Bias가 낮아서 사람보다 결과가 낫다!

<figure>
<img src="/assets/img/Screen Shot 2021-09-06 at 9.23.30 PM.png" alt="machine learning"></figure>

<figure>
<img src="/assets/img/Screen Shot 2021-09-06 at 9.23.41 PM.png" alt="deep learning">
<figcaption>Fig 4. Machine Learning과 Deep Learning의 차이</figcaption>
</figure>

# Classification

### Classifier

<figure>
<img src="/assets/img/Screen Shot 2021-09-06 at 9.36.08 PM.png" alt="deep learning" width="550">
<figcaption>Fig 5. Classifier example</figcaption>
</figure>

- 만일 우리가 세상 모든 데이터를 갖고 있다면?
    - k Nearest Neighbors (k-NN)을 통해 분류 문제를 풀 수 있다

🎈 k Nearest Neighbors란?\
Query datapoint의 주변 데이터들을 찾고 그 데이터들이 갖던 Label 정보들을 가져와서 분류하는 것! (e.g. 주변에 빨강이 많으니까 이건 빨강인가 보다~)

<br>
- 그렇게 되면 그저 검색해서 비슷한 것을 찾을 수 있게 된다
- 하지만 세상의 모든 데이터를 가질 수도 없고 데이터 양에 따라서 복잡도도 올라가니 비효율적이다
- 그래서 데이터를 압축시켜서 Neural Networks에 Parameter들을 녹여 넣는 것이 CNN의 핵심

### Convolutional Neaural Networks (CNN)

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 2.14.57 PM.png">
<figcaption>Fig 6. A simple perceptron model</figcaption>
</figure>

- CNN 중 가장 간단한 모델을 보자
- Perceptron that takes every pixel of an image as input (Fully-connected layer)
- 각 pixel에 서로 다른 가중치로 weighted sum, 즉 내적을 하고
- Acfivation function을 적용해서 분류 스코어로 출력

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 2.34.59 PM.png">
<figcaption>Fig 7. Visualization of single fully connected layer networks</figcaption>
</figure>

- Weight matrix를 영상구조에 맞춰서 reshape한다
- FCL를 입력 영상과 동일한 template이라고 볼 수 있다
- 이 template과의 내적을 통해 각 클래스의 스코어를 뽑아 비교한다
- Weight matrix를 이미지화 시키면 그 클래스의 평균 이미지가 나온다
- 단점으로는 layer가 단순해서 평균이미지 외에는 표현이 안된다

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 2.40.13 PM.png" width="500">
<figcaption>Fig 8. A prolbem with single fully connected layer networks</figcaption>
</figure>

- 적용 시점 즉, Test time application에서의 문제
- 만약 말의 이미지 중 일부분만 나오는 이미지가 입력되면 학습 때 본 적이 없기에 잘못된 결론을 내림

### Locally Connected Neural Networks

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 3.07.24 PM.png">
<figcaption>Fig 9. CNNs are locally connected neural networks</figcaption>
</figure>

- 원래 FCN는 하나의 특징을 뽑기 위해서 모든 펙셀들을 고려한다
    - pixel마다 weight가 있기 때문에 parameter 수가 급격히 증가한다
- 반면 LCN은 영상의 공간의 습성을 이해해서 국부적인 영역들의 connection을 고려한다
    - filter 이외의 영역에는 weight가 필요없으니 parameter 수가 줄어든다
- 영상 여러군데에서 비슷한 특징이 있을 수 있기에 전 영역에 connection을 공유하면서 필터가 순회하면서 feature를 뽑는다 -> sliding window 기법
- Hidden layer의 activation node를 feature map 형태로 잔뜩 뽑는다
- Parameter를 영상 전체에 재활용할 수 있고 더 적은 Parameter 수로 효과적인 특징을 추출할 수 있게 된다, 즉, Overfitting도 방지를 해준다
- 즉, LCN으로 어떠한 특징의 상대적 위치가 바뀌더라도 그 특징이 적절하게 잘 뽑히게 된다

# CNN Architectures for Image Classification 1

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 3.10.22 PM.png">
<figcaption>Fig 9. CNNs are locally connected neural networks</figcaption>
</figure>

### LeNet-5

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 3.18.49 PM.png">
<figcaption>Fig 10. LeNet-5</figcaption>
</figure>

- 1988년 제일 심플한 CNN 아키텍처 by 얀 리쿤 (Yann Lecun)
    - Overall architecture: Conv - Pool - Conv - Pool - FC - Fc
    - Convolution: 5x5 filters with stride 1
    - Pooling: 2x2 max pooling with stride 2
- 한글자 단위 인식, 우편 번호 인식에 크게 성공해서 우편 운송에 큰 기여를 했다

### AlexNet

- LeNet-5과 비슷하지만...
    - 더 크다 (7 hidden layers, 605k neurons, 60M parameters)
    - ImageNet (large amount of data, 12M)으로 훈련
    - ReLU와 dropout 기법을 사용했다

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 3.21.09 PM.png">
<figcaption>Fig 11. AlexNet</figcaption>
</figure>

- Ovreall architecture
    - Conv - Pool - LRN - Conv - Pool - LRN - Conv - Conv - Conv - Pool - FC - FC - FC
    - GPU 메모리가 모자라 2개로 나뉜다
    - Activation map이 cross하는데 GPU에 모든 부분에서 cross communication이 일어나면 느려지기 때문에 일부에서만 교환한다

🎈 LRN이란? 일종의 Normalization, Normalization은 activation function에 입력되기 전에 적용함으로서 값이 extreme해지는 것을 막아준다

> Normalization has become important for deep neural networks that compensate for the unbounded nature of certain activation functions such as ReLU, ELU, etc. With these activation functions, the output layers are not constrained within a bounded range (such as [-1,1] for tanh), rather they can grow as high as the training allows it. To limit the unbounded activation from increasing the output layer values, normalization is used just before the activation function.

참고: <a href="https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac" target="_blank">Local Response Normalization</a> / <a href="https://www.thoughtco.com/lateral-inhibition-4687368" target="_blank">Lateral Inhibition</a>

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 4.07.36 PM.png">
<figcaption>Fig 12. AlexNet 구현 예시 (LRN은 제외)</figcaption>
</figure>

- 마지막 max pooling된 2D activation map이 Linear layer로 가기위해서는 특별한 작업이 필요하다
- 바로 Tensor를 Vector로 바꿔줘야하는 작업!
- Average pooling: 공간 정보를 압축하여 긴 채널축의 정보만 남겨둠
- Flattening: Activation map을 어떠한 순서로 나열해서 벡터 모양으로 만들고 쌓아줌
- 두 개의 차이는 dimension 차이만 있을 뿐 벡터화 시켜주는 것은 같다

##### Deprecated Components

- LRN: 요즘은 Batch normalization을 더 많이 사용한다
- 11x11 convolution filter: 요즘은 이렇게 큰 filter size를 사용하지 않음

### Receptive Field in CNN

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 4.27.01 PM.png">
<figcaption>Fig 13. Receptive field in CNN</figcaption>
</figure>

- 한 output 값을 정하기 위해 input space에서 어느정도 영역을 차지하였느냐?
- 필터 사이즈가 클수록 더 넓은 영역을 참고한다
- Suppose KxK conv. filters with stride 1, and a pooling of size PxP -> value of each unit in the pooling layer depends on an input patch of size: **(P+K-1)x(PxK-1)**

### VGGNet

<figure>
<img src="/assets/img/Screen Shot 2021-09-22 at 4.37.09 PM.png" width="400">
<figcaption>Fig 14. VGG16 and VGG19</figcaption>
</figure>

- 더 깊어진 architecture (16 and 19 layers)
- 더 단순해진 architecture
    - LRN 사용 안함
    - 3x3 conv filters과 2x2 max pooling만 사용
- 더 나은 performance and generalization
- Input
    - 224x224 RGB images (AlexNet과 동일)
    - 평균 RGB값을 빼줌으로써 Normalization
- Key design choices
    - 3x3 conv filters with stride 1
    - 2x2 max pooling operations
- 3 fully-connected layers
- ReLU for non-linearity, No LRN

🎈 왜 3x3 conv layers만 사용했는가?
- 큰 receptive field를 유지할 수 있다
- 더 깊은 네트워크에 따라오는 more non-linearities
- 더 적은 parameter 수

