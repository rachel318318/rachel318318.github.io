---
layout: post
title: ✏️ U-stage | DL History & MLP
date: 2021-08-09 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 머신러닝 수학의 기초가 되는 개념들을 훑어보는 시간
tag:
  - Boostcamp AI
  - U-stage
  - DL Basics
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. Introduction
---
##### What makes you a good deep learner?

* Implementation skills
* Math skills
* Knowing/reading a lot of recent papers

##### Composition

AI -> Machine Learning -> Deep Learning\
Mimic human intelligence through data-driven approach using neural networks

##### Key Components of Deep Learning

* **Data** that the model can learn from
* A **model** that transforms the data
* A **loss function** that quantifies the error of the model
* An **algorithm** to adjust the parameters to minimize the loss

# 2. DL History
---
##### 2012: AlexNet
이미지 대회에 딥러닝을 최초로 적용하여 1등함\
막연히 생각만 하던 인간의 neural system을 모방하는 기술을 실제로 구현함

##### 2013: DQN
알파고 알고리즘 학습 방법으로 유명\
당시 스타트업이었던 DeepMind에서 구현함

##### 2014: Encoder/Decoder
NMT(Natural Machine Translation)을 구현하는 알고리즘\
구글 번역기가 쓰는 방법

##### 2014: Adam Optimizer

##### 2015: Generative Adversarial Network (GAN)
Les Trois Brasseurs라는 술집에서 술 먹다 술이 너무 맛이 없어 생각에 잠긴 와중 떠올린 시스템

##### 2015: Residual Networks
전에는 레이어를 쌓아도 어느 상한선을 넘기면 성능이 나빠졌지만 Residual Networks로 인하여 몇개를 쌓아도 성능이 좋아지는 모습을 보였다. 즉, 딥러닝을 제대로 구현하게 만든 최초의 시스템.

##### 2017: Transformer
구글에서 파격적인 제목으로 발표한 논문 (Attention Is All You Need)

##### 2018: BERT (fine-tuned NLP models)

##### 2019: BIG Language Models
일런 머스크의 OpenAI가 공개한 초거대 AI 'GPT-3'으로 유명

##### 2020: Self Supervised Learning
Label이 없는 이미지를 활용하기 위해 사용하는 방법

# 3. Neural Networks & Multi-layer Perceptron
---
사실상 Deep learning이 대충 사람의 뇌를 모방했다는 전제보다는 왜 잘되는지 수학적인 분석을 통해서 \
연구하는 것이 더 효율적일 것이다.

> Neural networks are function approximators that stack affine transformations \
followed by nonlinear transformations.

##### MLP (Multi-Layer Perceptron)

<figure>
<img src="/assets/img/IMG_1235.jpg" alt="MLP">
<figcaption>Fig 4. Multi-Layer Perceptron</figcaption>
</figure>

$$W$$ = Weight \
$$b$$ = Bias \
$$\rho$$ = Nonlinear transform

🎈 One way of interpreting a matrix is to regard it as a mapping between two vector spaces

\
자세한 내용은 [DL Intro]({% post_url 2021-08-03-DLintro %}) 참고