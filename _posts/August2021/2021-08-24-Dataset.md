---
layout: post
title: 💭 Data Class Comp | Dataset & Data Generation
date: 2021-08-24 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 이미지 분류 대회를 위한 모델을 만드는 과정
tag:
  - Boostcamp AI
  - P-stage
  - Data Classification Competition
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# Dataset

### Pre-processing

* 어떤 형태인지 모르는 데이터(Vanilla Data)를 모델이 좋아하는 형태의 Dataset으로 만들어 줘야함
* 80%의 시간이 데이터 전처리하는데에 쓰임
  * 노이즈, Null값 등 쓸 수 없는 데이터가 사실상 많음
  * 질 좋은 데이터를 모델에 넣어주는 것이 성능 개선의 핵심

* 예시들
  * Bounding Box: 원하는 객체만 뽑음
  * Resize: 다양한 사이즈 획일화, 큰 용량의 이미지 축소 -> 작업의 효율화

### Generalization

* Underfitting: High bias, Overfitting: High variance
* Validation set: 학습한 모델 검증하기 위해 train set 중 일정 부분을 뽑아 놓음

### Data Augmentation

* Generalization의 한 방법
* 주어진 데이터가 가질 수 있는 경우(case)와 상태(state)의 다양성을 고려함
* 즉, 노이즈가 있는 데이터를 보더라도 Robust하게 처리 가능
* `torchvision.transforms` 활용
  * 과연 의미있는 변수인지도 고려하기, 무의미한 처리 방식은 사용하지 않기
  * ex. 마스크 이미지를 위아래로 flip

```py
transforms.Compose([
  Resize((96,128), Image.BILINEAR),
  RandomRotation([-8,+8]),
  ...
  ToTensor(),
  Normalize(mean=self.mean, std=self.std),
  AddGaussianNoise(0.,1.)
])

image = self.read_image(index)
image_transform = self.transform(image)
```

* `albumentations`: 더 빠르고 다양한 transform 제공하는 패키지 (download from Github)

### ❗️Caution

* There is no absolute answer for data augmentation!
* You need to prove validity of the augmentation through multiple experiments

# Data Generation

### Data Feeding

* 먹이를 주다 = 대상의 상태를 고려해서 적정한 양을 준다
* 모델이 처리할 수 있는 데이터양을 고려해야함
* 모델의 처리량만큼 데이터를 generate하는가?

<figure>
<img src="/assets/img/IMG_1267.jpg" alt="Feeding">
<figcaption>Fig 1. Data generator와 Model의 데이터 처리량 비교</figcaption>
</figure>

<figure>
<img src="/assets/img/IMG_1268.jpg" alt="Feeding">
<figcaption>Fig 2. Dataset 생성 능력 비교</figcaption>
</figure>

* 어떠한 변형은 모델의 성능을 낮추기도 함
* 여러가지 조합을 해보면서 튜닝과정이 필요함

### torch.utils.data

* **Dataset** 구조

```py
from torch.utils.data import Dataset

class myDataset(Dataset): # Dataset 라이브러리 상속
  def __init__(self): # MyDataset 클래스가 처음 선언되었을 때 호출
    pass
  
  def __getitem__(self,index): # MyDataset의 데이터 중 index 위치의 아이템을 리턴
    return None
  
  def __len__(self): # MyDataset 아이템의 전체 길이
    return None
```

* **DataLoader**: 내가 만든 Dataset을 효율적으로 사용할 수 있도록 관련 기능 추가
* 굉장히 많은 기능들을 제공
  * shuffle
  * sampler
  * collate_fn 등

```py
train_loader = torch.utils.data.DataLoader(
  train_set,
  batch_size = batch_size,
  num_workers = num_workers,
  drop_last = True
)

# -> (Batch, Channel, Height, Width)
```

* Dataset과 DataLoader는 **분리되는 것**이 좋음
* 재사용이 가능해지고 효율이 올라가기 때문