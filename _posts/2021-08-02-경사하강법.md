---
layout: post
title: ✏️ U-stage | 경사하강법
date: 2021-08-02 16:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 머신러닝 수학의 기초가 되는 개념들을 훑어보는 시간
tag:
  - Boostcamp AI
  - U-stage
  - AI math basics
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. 경사하강법(순한맛)

### 미분

* 변수의 움직임에 따른 함수값의 변화를 측저하기 위한 도구
* 최적화에서 제일 많이 사용하는 기법

$$f'(x)=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}$$

* `sympy.diff`로 계산 가능

```py
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3), x)
```

* 미분은 함수 $$f$$의 주어진 점 $$(x,f(x))$$에서의 접선의 기울기를 구함
* 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있음

<figure>
<img src="/assets/img/IMG_1181.jpg" alt="접선의 기울기">
<figcaption>Fig 1. (x,f(x))에서의 접선의 기울기</figcaption>
</figure>

* 미분값을 더하면 경사상승법(gradient ascent), 미분값을 빼면 경사하강법(gradient descent)
* 각 함수의 극대값과 극소값의 위치를 구할 때 사용함

### 경사하강법: 알고리즘

```py
# gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while abs(grad) > eps:
  var = var - lr * grad
  grad = gradient(var)
```

🎈 컴퓨터로 미분이 정확히 0이 되는 것은 불가능하므로 `eps`보다 작을 때 종료하는 조건이 필요

🎈 `lr`은 학습률로서 미분을 통해 업데이트하는 속도를 조절함

### 변수가 벡터이면?

* 다변수 함수인 경우 편미분(partial differentiation)을 사용

$$\partial_{x_{i}}f(\textbf{x})=\lim_{h\rightarrow0}\frac{f(\textbf{x}+h\textbf{e}_{i})-f(\textbf{x})}{h}$$

* $$\textbf{e}_{i}$$는 $$i$$번째 값만 1이고 나머지는 0인 단위벡터
* 그레디언트(gradient) 벡터를 이용하여 경사하강/경사상승법에 적용

$$\nabla f=(\partial_{x_{1}}f,\partial_{x_{2}}f,\cdots,\partial_{x_{d}}f)$$

```py
import sympy as sym
from sympy.abc import x,y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)
# >> 2*x + 2*y - sin(x + 2*y)
```

<figure>
<img src="/assets/img/IMG_1182.jpg" alt="그레디언트 벡터">
<figcaption>Fig 2. 그레디언트 벡터</figcaption>
</figure>

```py
# gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while norm(grad) > eps: #abs()대신 norm()써서 종료조건 설정
  var = var - lr * grad
  grad = gradient(var)
```

# 2. 경사하강법(매운맛)

### 선형회귀분석 복습

* 전 포스팅에 나왔던 <a href="/벡터와행렬/#응용2-선형회귀분석">선형회귀분석</a> 문제 참고
* 역행렬 대신 경사하강법으로 선형모델을 찾자

### 선형회귀 계수 구하기

$$\nabla_{\beta}\left\| y-\textbf{X}\beta \right\|_{2} = (\partial_{\beta_{1}}\left\| y-\textbf{X}\beta \right\|_{2},\dots,\partial_{\beta_{d}}\left\| y-\textbf{X}\beta \right\|_{2})$$

$$\partial_{\beta_{k}}\left\| y-\textbf{X}\beta \right\|_{2}=\partial_{\beta_{k}}\bigg\{\frac{1}{n}\sum_{i=1}^{n}\bigg(y_{i}-\sum_{j=1}^{d}X_{ij}\beta_{j}\bigg)^2\bigg\}^{1/2}$$

* 최소화하는 $$\beta$$를 찾아야 함
* 위 식을 계산하면...

$$-\frac{\textbf{X}^{\top}(\textbf{y}-\textbf{X}\beta)}{n\|\textbf{y}-\textbf{X}\beta\|_{2}}$$

* $$\textbf{X}\beta$$를 계수 $$\beta$$에 대해 미분한 결과인 $$\textbf{X}^{\top}$$만 곱해진 것

🤔 어떻게 계산하는지 알아낼 것...

<br/>

* 제곱하면 식이 더 간단해진다

$$\nabla_{\beta}\left\| y-\textbf{X}\beta \right\|_{2}^{2} = -\frac{2}{n}\textbf{X}^{\top}(\textbf{y}-\textbf{X}\beta)$$

* 목적식을 최소화하는 $$\beta$$를 구하는 경사하강법 알고리즘은...

$$\beta^{(t+1)}\leftarrow\beta^{t}-\lambda\nabla_{\beta}\|\textbf{y}-\textbf{X}\beta^{(t)}\|$$

* 둘째 $$\lambda$$-term에 대입하면 끝!

```py
# norm: L2-노름을 계산하는 함수
# lr: 학습률, T: 학습횟수

for t in range(T): # 종료조건을 일정 학습횟수로 변경
  error = y - X @ beta
  grad = - transpose(X) @ error
  beta = beta - lr * grad
```

🎈 학습률과 학습횟수가 중요해진다!

### 비선형회귀 문제

* 목적식이 볼록하지 않을 수 있기 때문에 경사하강법이 항상 수렴보장x
* 확률적 경사하강법(stochastic gradient descent):
  * 모든 데이터를 사용하는 대신 데이터 한개 또는 일부를 활용하여 업데이트
* 미니배치(batch)는 확률적으로 선택하므로 목적식 모양이 계속 바뀜

<figure>
<img src="/assets/img/IMG_1184.jpg" alt="확률적 경사하강법">
<figcaption>Fig 3. 확률적 경사하강법과 다양한 목적식</figcaption>
</figure>

<figure>
<img src="/assets/img/IMG_1185.jpg" alt="확률적 경사하강법 원리">
<figcaption>Fig 4. 확률적 경사하강법 원리</figcaption>
</figure>