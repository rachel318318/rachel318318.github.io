---
layout: post
title: ✏️ U-stage | Deep Learning Intro
date: 2021-08-03 10:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 머신러닝 수학의 기초가 되는 개념들을 훑어보는 시간
tag:
  - Boostcamp AI
  - U-stage
  - AI math basics
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. 신경망

<figure>
<img src="/assets/img/IMG_1187.jpg" alt="신경망수식">
<figcaption>Fig 1. 데이터 X와 가중치 행렬 W, 절편 b 벡터로 표현된 신경망수식</figcaption>
</figure>

<figure>
<img src="/assets/img/IMG_1188.jpg" alt="신경망모델" width="450">
<figcaption>Fig 2. 신경망 모델</figcaption>
</figure>

* $$d$$개의 변수로 $$p$$개의 선형모델을 만들어서 $$p$$개의 잠재변수를 설명하는 모델로 가정

### 소프트맥스 연산
* 모델의 출력을 확률(특정한 클래스에 속할 확률)로 해석할 수 있게 변환해주는 연산
* 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합, 예측

```py
def softmax(vec):
    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
    numerator = np.sum(denumerator, axis=-1, keepdims=True)
    val = denumerator / numerator
    return val

vec = np.array([[1,2,0],[-1,0,1],[-10,0,10]])
softmax(vec)

'''
array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],
       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],
       [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])
'''
```

🤔 여기서 axis=-1은 무슨 뜻? 그리고 무슨 특정한 클래스인가?

### 활성함수(activation function)

$$\textbf{H}=(\sigma(\textbf{z}_{1}),\dots,\sigma(\textbf{z}_{n}))$$

$$\sigma(\textbf{z})=\sigma(\textbf{Wx}+\textbf{b})$$

<figure>
<img src="/assets/img/IMG_1190.jpg" alt="신경망" width="350">
<figcaption>Fig 3. 활성함수를 적용한 신경망</figcaption>
</figure>

* 비선형 함수(nonlinear)로서 딥러닝에서 매우 중요한 개념
* 딥러닝에선 ReLU함수를 많이 쓰고 있음

🤔 활성함수를 정확히 왜 써야하는지?

<figure>
<img src="/assets/img/IMG_1189.jpg" alt="활성함수 종류">
<figcaption>Fig 4. sigmoid, tanh, ReLU 함수</figcaption>
</figure>

* 다층(multi-layer) 퍼셉트론(MLP)은 신경망이 여러층 합성된 함수
* $$l = 1,\dots,L$$까지 순차적인 신경망 계산을 순전파(forward propagation)이라 부름

<figure>
<img src="/assets/img/IMG_1193.jpg" alt="다층 퍼셉트론">
<figcaption>Fig 5. 다층 신경망/퍼셉트론</figcaption>
</figure>

🎈 왜 층을 여러개 쌓나요?
* 층이 깊을수록 목적함수를 간사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 <br>좀 더 효율적으로 학습가능

### 역전파 알고리즘

* 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 사용된 패러미터를 학습
* 각 층 패러미터의 그레디언트 벡터는 윗층부터 역순으로 계산

<figure>
<img src="/assets/img/IMG_1195.jpg" alt="역전파 알고리즘">
<figcaption>Fig 6. 역전파 알고리즘</figcaption>
</figure>

🤔 예제 문제 더 이해하기, 손실 함수는 또 무엇인가

