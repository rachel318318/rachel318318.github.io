---
layout: post
title: 🌱 Pre-course | 통계학
date: 2021-07-29 15:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 머신러닝 수학의 기초가 되는 개념들을 훑어보는 시간
tag:
  - Boostcamp AI
  - Pre-course
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 통계학(Statistics)

### 모수란?

* 통계적 모델링은 적절한 가정 위에서 **확률분포**를 추정(inference)하는 것이 목표
* 정확하게 알아내는 것은 불가능하므로, 근사적으로 추정해야함
    * 데이터와 추정방법의 불확실성을 고려해서 위험을 최소화
* **모수(parametric) 방법론**: 데이터가 특정 확률분포를 따른다고 가정한 후 모수를 추정
* **비모수(nonparametric) 방법론**: 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조, <br>모수의 개수가 바뀜
* 기계적으로 확률분포를 가정해서는 안됨!
    * 각 분포마다 모수 추정한 뒤 검정하는 방법들이 있음

### 표집분포(Sampling distribution)

* 정규분포의 모수: 평균(population mean) $$\mu$$과 분산(population variance) $$\sigma^2$$
* 아래는 Sample mean과 Sample variance

$$\bar{X} = \frac{1}{N}\sum_{i=1}^{N}X_{i}$$

$$S^2 = \frac{1}{N-1}\sum_{i=1}^{N}(X_{i}-\bar{X})^2$$

* 통계량의 확률분포를 **표집분포(sampling distribution)**라 부름
* $$N$$이 커질수록 $$\textit{N}(\mu,\sigma^2/N)$$를 따름
    * **중심극한정리(Central Limit Theorem)**이라 지칭
    * 모집단(Sample or population)의 분포가 정규분포를 따르지 않아도 성립

### 최대가능도 추정법

* 최대가능도 추정법(maximum likelihood estimation, MLE): 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나

$$\hat{\theta}_{MLE}=argmax_{\theta}\:L(\theta;\textbf{x}) = argmax_{\theta}\:P(\textbf{x}\vert\theta)$$

🎈 가능도(likelihood)함수는 모수 $$\theta$$를 따르는 분포가 $$\textbf{x}$$를 관찰할 가능성을 뜻하지만<br>
**확률로 해석하면 안됨!**<br>
확률밀도함수, 확률질량함수와 같은 표현이지만... $$\theta$$에 따른 배속 효과로 봐야함. 다 합해서 1이 아님!

<br/>

* 데이터 집합 $$\textbf{X}$$가 독립적으로 추출되었을 경우 로그가능도를 최적화합니다

<figure>
<img src="/assets/img/IMG_1200.jpg" alt="로그가능도">
<figcaption>Fig 1. 로그가능도 최적화</figcaption>
</figure>

🎈 왼쪽 식은 확률밀도함수, 확률질량함수의 곱으로 표현

##### 왜 로그가능도를 사용하는가?

* 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도 계산하는 것은 불가능
* 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용
    * 로그 가능도를 사용하면 $$O(N^2)$$에서 $$O(n)$$으로 줄여짐
* 대개의 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 <br>최적화하게 됨

### 최대가능도 추정법 예제: 정규분포

❓ 정규분포를 따르는 확률변수 $$X$$로부터 독립적인 표본 $$\{x_{1},\dots,x_{n}\}$$을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하면?

<figure>
<img src="/assets/img/IMG_1201.jpg" alt="예제1-1">
</figure>
<figure>
<img src="/assets/img/IMG_1203.jpg" alt="예제1-2" width="400">
</figure>
<figure>
<img src="/assets/img/IMG_1204.jpg" alt="예제1-3"><figcaption>Fig 2. 최대가능도 예제: 정규분포</figcaption>
</figure>

1. $$\frac{\partial log L}{\partial\mu}$$와 $$\frac{\partial log L}{\partial\sigma}$$를 구해 0이 되게 한다.
2. 두 미분이 모두 0이 되는 $$\mu,\sigma$$를 찾으면 가능도가 최대화 된다.
3. 계산된 $$\mu,\sigma^2$$은 다음과 같다.

🎈 구해진 $$\sigma^2$$와 같이 MLE는 불편추정량($$\frac{1}{n-1}$$)을 보장x

### 최대가능도 추정법 예제: 카테고리 분포

❓ 카테고리 분포 $$Multinoulli(\textbf{x};p_{1},\dots,p_{d})$$를 따르는 확률변수 $$X$$로부터 독립적인 표본 $$\{x_{1},\dots,x_{n}\}$$을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하면?

<figure>
<img src="/assets/img/IMG_1205.jpg" alt="예제1-3"><figcaption>Fig 3. 촤대가능도 예제: 카테고리 분포</figcaption>
</figure>

1. d차원 까지의 p를 모두 더하면 1이라는 제약식을 만족해야한다.
2. 로그가능도, 라그랑주 승수법을 이용해 식을 구한다.
3. $$\frac{\partial L}{\partial p_{k}}$$, $$\frac{\partial L}{\partial \lambda}$$를 구해 0이 되게 한다.
4. 답으로 카테고리 분포의 MLE는 경우의수를 세어서 비율 구하는 것.

자세한 건 ppt자료 참고...

### 딥러닝에서 최대가능도 추정법

* 딥러닝 모델의 가중치를 $$\theta=(\textbf{W}^{(1)},\dots,\textbf{W}^{(L)})$$라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 $$(p_{1},\dots,p_{d})$$를 모델링함
* 원핫벡터로 표현한 정답레이블 $$\textbf{y}=(y_{1},\dots,y_{K})$$을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있음

<figure>
<img src="/assets/img/IMG_1206.jpg" alt="딥러닝 최대가능도"><figcaption>Fig 4. 딥러닝에서의 최대가능도 추정법</figcaption>
</figure>

* 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 <br>거리를 통해 유도
* 두 개의 확률분포 $$P(\textbf{x}),\,Q(\textbf{x})$$가 있을 경우 두 확률분포 사이의 거리를 계산할 때...
    * 총변동 거리 (Total Variation Distance, TV)
    * 쿨백-라이블러 발산 (Kullback_Leibler Divergence, KL)
    * 바슈타인 거리 (Wasserstein Distance)

$$KL(P\|Q)=-E_{\textbf{x} \sim P(\textbf{x})}[\log Q(\textbf{x})] + E_{\textbf{x} \sim P(\textbf{x})}[\log P(\textbf{x})]$$

* 첫번째 term은 크로스 엔트로피, 두번째 term은 엔트로피
* 분류 문제에서 정답레이블을 $$P$$, 모델 예측을 $$Q$$라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같음
* 거리를 최소화 = 로그가능도 최대화

🎈 $$KL(P\|Q) \neq KL(Q\|P)$$