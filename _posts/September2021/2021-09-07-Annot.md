---
layout: post
title: ✏️ U-stage CV | Annotation & Data Efficient Learning
date: 2021-09-07 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: Computer Vision에 대한 심화 학습
tag:
  - Boostcamp AI
  - U-stage Computer Vision
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# Data Augmentation

### Learning representation of dataset

* 우리가 가지는 데이터셋은 대부분 biased 되어있다
* 사람이 카메라로 구도를 잡고 예쁘게 찍어려고 하기 때문
* 즉, 데이터셋이 리얼 데이터를 충분히 반영하지 않는 것이 문제
* 그래서 Data Augmentation이 이러한 갭을 매꾸기 위해 존재

### Augmentation 종류와 쓰임

* OpenCV와 NumPy에서 여러가지 augmentation 함수 제공
* Brightness, Rotate, Flip, Crop 등...
* Affine transformation
    * preserves 'line', 'length ratio', and 'parallelism' in image
    * 예) 사각형을 평행사변형으로 바꾸는 것
* CutMix
    * 두 개 이상?의 이미지를 특정 비율로 잘라서 합치는 것
    * 라벨도 잘라서 넣어주는 것이 특징
    * 성능도 향상되고 물체의 위치를 더 정교하게 파악
* RandAugment
    * 어떤 조합을 해야 성능이 더 잘 나올까?
    * 자동으로 베스트 조합을 찾아준다
    * Policy: RandAugment에서 조합 또는 시퀀스를 이렇게 부름
    * 두 개의 변수: 어떤 augmentation과 얼마나 세게?

🎈 코드 예시는 ppt 자료 참고

### Leveraging pre-trained information

* Labeled 데이터로 학습하는 supervised learning은 방대항 양의 데이터를 요구한다
* 사람이 직접하는 레이블링은 비싸기도하고 퀄리티도 좋지 않다
* 그래서 small dataset을 이용하고도 성능이 잘 나오게 하는 **Transfer Learning**을 도입해야한다

### Transfer learning

* 비슷한 데이터셋은 같은 정보를 공유할 가능성이 높다
* 그래서 비슷한 데이터로 학습한 pre-trained model을 가져와서 다시 목적에 맞게 학습할 수 있다

##### Approach 1

* Pre-trained model에서 FC layer는 잘라내고 새로운 FC layer를 다시 학습시킨다
* 기존에 학습된 지식을 갖고 있으면서 새로운 Task에 대응하도록 학습한다
* 또한 학습해야 되는 Parameter 수가 적어지므로 적은 데이터로도 OK

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.11.39 PM.png" alt="pre-trained fc cut off">
<figcaption>Fig 1. Pre-trained model implementation</figcaption>
</figure>

##### Approach 2

* Pre-trained model에서 FC layer는 마찬가지로 잘라낸다
* Convolution layers에는 작은 learning rate를, 새로운 FC layer에는 더 큰 learning rate를 준다
* 조금 더 유연해지고 새로운 Target task에 더 빠르게 적응할 수 있게 한다

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.11.47 PM.png" alt="pre-trained diff lr" width="300">
<figcaption>Fig 2. Differenct learning rates for CL and FC</figcaption>
</figure>

### Knowledge distillation

* Student model이 Teacher model의 아웃풋을 따라하게 만드는 것
* 보통 student model이 더 작은 네트워크이다
* Unsupervised learning 즉, 레이블 없이도 가능하다
* KL divergence loss는 두 출력의 distribution을 비슷하게 만드는 역할
* gradient는 student model에만 도달하고 student model만 학습한다
* 만일 labeled data가 있다면 학습 구조가 바뀌게 된다

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.11.56 PM.png" alt="kowledge dist no label">
<figcaption>Fig 3. Knowledge distillation with no label</figcaption>
</figure>

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.12.08 PM.png" alt="knowledge dist with labels">
<figcaption>Fig 4. Knowledge distillation with true labels</figcaption>
</figure>

##### Hard label vs. Soft label

* Hard label (One-hot vector)
    * Typically obtained from the dataset
    * 1과 0으로만 클래스가 true answer인지 아닌지 표현
* Soft label
    * Typically output of the model (=inference result)
    * 모델이 어떠한 입력을 보고 어떠한 생각을 하는지 알아보기 위해 쓰임
    * 실수값으로 표현: 0,1보다는 실수가 지식을 표현하는데 더 유용하다

##### Softmax with temperature (T)

* 출력에서 큰 값과 작은 값의 차이를 조절하는데 쓰인다
    * 일반 softmax는 극단적으로 값들을 벌려놓는다
* T가 높을수록 차이를 더 줄여준다
* 전반적인 분포를 볼 수 있기 때문에 더 많은 정보를 가지게 된다

##### Semantic information not considered

* Teacher model과 student model의 아웃풋이나 Task는 다를 수 있다
* 전체의 개형이 추상적인 지식을 표현하기 때문에 그 행동을 따라하는 것이 중요하지 Teacher model에 내포되어있는 의미들은 상관이 없다

##### Losses

* Distillation loss
    * KLdiv(Soft label, Soft prediction)
    * Loss = 선생과 학생 모델 inference의 차이
    * 선생 모델의 지식을 학생 모델이 학습하는 것
* Student loss
    * CrossEntropy(Hard label, Soft prediction)
    * Loss = 학생 inference와 true label의 차이
    * 정답을 학습하는 것
* 이 두 loss들의 weighted sum을 통해 학생 모델에만 backpropagation한다 (학생 모델만 학습)

### Leveraging unlabeled dataset for training

# Leveraging unlabeled dataset for training

### Semi-supervised learning

* 웹상에 있는 데이터들은 대부분 No label이다
* 이것만 활용을 잘 한다면 어마어마한 데이터를 얻을 수 있을 것
* Semi-supervised learning: Unsupervised (No label) + Fully Supervised (Fully labeled)

### Pseudo labeling

* Pre-trained 모델에서 unlabeled data들을 추론해서 그걸 label로 붙힌다
    * 그렇게 해서 label이 붙혀진 dataset이 pseudo-labeled dataset이다
* Labeled dataset과 Pseudo-labeled dataset 동시에 다른 모델에 학습시킨다
    * 다음 단계의 모델은 조금씩 커진다
* Self-training이라는 연구 참고

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.14.13 PM.png" alt="pseudo-labeled" width="500">
<figcaption>Fig 5. Training with labeled and pseudo-labeled data</figcaption>
</figure>

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.14.30 PM.png" alt="self-training">
<figcaption>Fig 5. Self-training structure</figcaption>
</figure>