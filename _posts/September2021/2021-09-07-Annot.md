---
layout: post
title: âœï¸ U-stage CV | Annotation & Data Efficient Learning
date: 2021-09-07 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: Computer Visionì— ëŒ€í•œ ì‹¬í™” í•™ìŠµ
tag:
  - Boostcamp AI
  - U-stage Computer Vision
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# Data Augmentation

### Learning representation of dataset

* ìš°ë¦¬ê°€ ê°€ì§€ëŠ” ë°ì´í„°ì…‹ì€ ëŒ€ë¶€ë¶„ biased ë˜ì–´ìˆë‹¤
* ì‚¬ëŒì´ ì¹´ë©”ë¼ë¡œ êµ¬ë„ë¥¼ ì¡ê³  ì˜ˆì˜ê²Œ ì°ì–´ë ¤ê³  í•˜ê¸° ë•Œë¬¸
* ì¦‰, ë°ì´í„°ì…‹ì´ ë¦¬ì–¼ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ë¬¸ì œ
* ê·¸ë˜ì„œ Data Augmentationì´ ì´ëŸ¬í•œ ê°­ì„ ë§¤ê¾¸ê¸° ìœ„í•´ ì¡´ì¬

### Augmentation ì¢…ë¥˜ì™€ ì“°ì„

* OpenCVì™€ NumPyì—ì„œ ì—¬ëŸ¬ê°€ì§€ augmentation í•¨ìˆ˜ ì œê³µ
* Brightness, Rotate, Flip, Crop ë“±...
* Affine transformation
    * preserves 'line', 'length ratio', and 'parallelism' in image
    * ì˜ˆ) ì‚¬ê°í˜•ì„ í‰í–‰ì‚¬ë³€í˜•ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê²ƒ
* CutMix
    * ë‘ ê°œ ì´ìƒ?ì˜ ì´ë¯¸ì§€ë¥¼ íŠ¹ì • ë¹„ìœ¨ë¡œ ì˜ë¼ì„œ í•©ì¹˜ëŠ” ê²ƒ
    * ë¼ë²¨ë„ ì˜ë¼ì„œ ë„£ì–´ì£¼ëŠ” ê²ƒì´ íŠ¹ì§•
    * ì„±ëŠ¥ë„ í–¥ìƒë˜ê³  ë¬¼ì²´ì˜ ìœ„ì¹˜ë¥¼ ë” ì •êµí•˜ê²Œ íŒŒì•…
* RandAugment
    * ì–´ë–¤ ì¡°í•©ì„ í•´ì•¼ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ê¹Œ?
    * ìë™ìœ¼ë¡œ ë² ìŠ¤íŠ¸ ì¡°í•©ì„ ì°¾ì•„ì¤€ë‹¤
    * Policy: RandAugmentì—ì„œ ì¡°í•© ë˜ëŠ” ì‹œí€€ìŠ¤ë¥¼ ì´ë ‡ê²Œ ë¶€ë¦„
    * ë‘ ê°œì˜ ë³€ìˆ˜: ì–´ë–¤ augmentationê³¼ ì–¼ë§ˆë‚˜ ì„¸ê²Œ?

ğŸˆ ì½”ë“œ ì˜ˆì‹œëŠ” ppt ìë£Œ ì°¸ê³ 

### Leveraging pre-trained information

* Labeled ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” supervised learningì€ ë°©ëŒ€í•­ ì–‘ì˜ ë°ì´í„°ë¥¼ ìš”êµ¬í•œë‹¤
* ì‚¬ëŒì´ ì§ì ‘í•˜ëŠ” ë ˆì´ë¸”ë§ì€ ë¹„ì‹¸ê¸°ë„í•˜ê³  í€„ë¦¬í‹°ë„ ì¢‹ì§€ ì•Šë‹¤
* ê·¸ë˜ì„œ small datasetì„ ì´ìš©í•˜ê³ ë„ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ê²Œ í•˜ëŠ” **Transfer Learning**ì„ ë„ì…í•´ì•¼í•œë‹¤

### Transfer learning

* ë¹„ìŠ·í•œ ë°ì´í„°ì…‹ì€ ê°™ì€ ì •ë³´ë¥¼ ê³µìœ í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤
* ê·¸ë˜ì„œ ë¹„ìŠ·í•œ ë°ì´í„°ë¡œ í•™ìŠµí•œ pre-trained modelì„ ê°€ì ¸ì™€ì„œ ë‹¤ì‹œ ëª©ì ì— ë§ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤

##### Approach 1

* Pre-trained modelì—ì„œ FC layerëŠ” ì˜ë¼ë‚´ê³  ìƒˆë¡œìš´ FC layerë¥¼ ë‹¤ì‹œ í•™ìŠµì‹œí‚¨ë‹¤
* ê¸°ì¡´ì— í•™ìŠµëœ ì§€ì‹ì„ ê°–ê³  ìˆìœ¼ë©´ì„œ ìƒˆë¡œìš´ Taskì— ëŒ€ì‘í•˜ë„ë¡ í•™ìŠµí•œë‹¤
* ë˜í•œ í•™ìŠµí•´ì•¼ ë˜ëŠ” Parameter ìˆ˜ê°€ ì ì–´ì§€ë¯€ë¡œ ì ì€ ë°ì´í„°ë¡œë„ OK

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.11.39 PM.png" alt="pre-trained fc cut off">
<figcaption>Fig 1. Pre-trained model implementation</figcaption>
</figure>

##### Approach 2

* Pre-trained modelì—ì„œ FC layerëŠ” ë§ˆì°¬ê°€ì§€ë¡œ ì˜ë¼ë‚¸ë‹¤
* Convolution layersì—ëŠ” ì‘ì€ learning rateë¥¼, ìƒˆë¡œìš´ FC layerì—ëŠ” ë” í° learning rateë¥¼ ì¤€ë‹¤
* ì¡°ê¸ˆ ë” ìœ ì—°í•´ì§€ê³  ìƒˆë¡œìš´ Target taskì— ë” ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆê²Œ í•œë‹¤

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.11.47 PM.png" alt="pre-trained diff lr" width="300">
<figcaption>Fig 2. Differenct learning rates for CL and FC</figcaption>
</figure>

### Knowledge distillation

* Student modelì´ Teacher modelì˜ ì•„ì›ƒí’‹ì„ ë”°ë¼í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ
* ë³´í†µ student modelì´ ë” ì‘ì€ ë„¤íŠ¸ì›Œí¬ì´ë‹¤
* Unsupervised learning ì¦‰, ë ˆì´ë¸” ì—†ì´ë„ ê°€ëŠ¥í•˜ë‹¤
* KL divergence lossëŠ” ë‘ ì¶œë ¥ì˜ distributionì„ ë¹„ìŠ·í•˜ê²Œ ë§Œë“œëŠ” ì—­í• 
* gradientëŠ” student modelì—ë§Œ ë„ë‹¬í•˜ê³  student modelë§Œ í•™ìŠµí•œë‹¤
* ë§Œì¼ labeled dataê°€ ìˆë‹¤ë©´ í•™ìŠµ êµ¬ì¡°ê°€ ë°”ë€Œê²Œ ëœë‹¤

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.11.56 PM.png" alt="kowledge dist no label">
<figcaption>Fig 3. Knowledge distillation with no label</figcaption>
</figure>

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.12.08 PM.png" alt="knowledge dist with labels">
<figcaption>Fig 4. Knowledge distillation with true labels</figcaption>
</figure>

##### Hard label vs. Soft label

* Hard label (One-hot vector)
    * Typically obtained from the dataset
    * 1ê³¼ 0ìœ¼ë¡œë§Œ í´ë˜ìŠ¤ê°€ true answerì¸ì§€ ì•„ë‹Œì§€ í‘œí˜„
* Soft label
    * Typically output of the model (=inference result)
    * ëª¨ë¸ì´ ì–´ë– í•œ ì…ë ¥ì„ ë³´ê³  ì–´ë– í•œ ìƒê°ì„ í•˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ì“°ì„
    * ì‹¤ìˆ˜ê°’ìœ¼ë¡œ í‘œí˜„: 0,1ë³´ë‹¤ëŠ” ì‹¤ìˆ˜ê°€ ì§€ì‹ì„ í‘œí˜„í•˜ëŠ”ë° ë” ìœ ìš©í•˜ë‹¤

##### Softmax with temperature (T)

* ì¶œë ¥ì—ì„œ í° ê°’ê³¼ ì‘ì€ ê°’ì˜ ì°¨ì´ë¥¼ ì¡°ì ˆí•˜ëŠ”ë° ì“°ì¸ë‹¤
    * ì¼ë°˜ softmaxëŠ” ê·¹ë‹¨ì ìœ¼ë¡œ ê°’ë“¤ì„ ë²Œë ¤ë†“ëŠ”ë‹¤
* Tê°€ ë†’ì„ìˆ˜ë¡ ì°¨ì´ë¥¼ ë” ì¤„ì—¬ì¤€ë‹¤
* ì „ë°˜ì ì¸ ë¶„í¬ë¥¼ ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§€ê²Œ ëœë‹¤

##### Semantic information not considered

* Teacher modelê³¼ student modelì˜ ì•„ì›ƒí’‹ì´ë‚˜ TaskëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤
* ì „ì²´ì˜ ê°œí˜•ì´ ì¶”ìƒì ì¸ ì§€ì‹ì„ í‘œí˜„í•˜ê¸° ë•Œë¬¸ì— ê·¸ í–‰ë™ì„ ë”°ë¼í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ì§€ Teacher modelì— ë‚´í¬ë˜ì–´ìˆëŠ” ì˜ë¯¸ë“¤ì€ ìƒê´€ì´ ì—†ë‹¤

##### Losses

* Distillation loss
    * KLdiv(Soft label, Soft prediction)
    * Loss = ì„ ìƒê³¼ í•™ìƒ ëª¨ë¸ inferenceì˜ ì°¨ì´
    * ì„ ìƒ ëª¨ë¸ì˜ ì§€ì‹ì„ í•™ìƒ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ê²ƒ
* Student loss
    * CrossEntropy(Hard label, Soft prediction)
    * Loss = í•™ìƒ inferenceì™€ true labelì˜ ì°¨ì´
    * ì •ë‹µì„ í•™ìŠµí•˜ëŠ” ê²ƒ
* ì´ ë‘ lossë“¤ì˜ weighted sumì„ í†µí•´ í•™ìƒ ëª¨ë¸ì—ë§Œ backpropagationí•œë‹¤ (í•™ìƒ ëª¨ë¸ë§Œ í•™ìŠµ)

### Leveraging unlabeled dataset for training

# Leveraging unlabeled dataset for training

### Semi-supervised learning

* ì›¹ìƒì— ìˆëŠ” ë°ì´í„°ë“¤ì€ ëŒ€ë¶€ë¶„ No labelì´ë‹¤
* ì´ê²ƒë§Œ í™œìš©ì„ ì˜ í•œë‹¤ë©´ ì–´ë§ˆì–´ë§ˆí•œ ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒ
* Semi-supervised learning: Unsupervised (No label) + Fully Supervised (Fully labeled)

### Pseudo labeling

* Pre-trained ëª¨ë¸ì—ì„œ unlabeled dataë“¤ì„ ì¶”ë¡ í•´ì„œ ê·¸ê±¸ labelë¡œ ë¶™íŒë‹¤
    * ê·¸ë ‡ê²Œ í•´ì„œ labelì´ ë¶™í˜€ì§„ datasetì´ pseudo-labeled datasetì´ë‹¤
* Labeled datasetê³¼ Pseudo-labeled dataset ë™ì‹œì— ë‹¤ë¥¸ ëª¨ë¸ì— í•™ìŠµì‹œí‚¨ë‹¤
    * ë‹¤ìŒ ë‹¨ê³„ì˜ ëª¨ë¸ì€ ì¡°ê¸ˆì”© ì»¤ì§„ë‹¤
* Self-trainingì´ë¼ëŠ” ì—°êµ¬ ì°¸ê³ 

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.14.13 PM.png" alt="pseudo-labeled" width="500">
<figcaption>Fig 5. Training with labeled and pseudo-labeled data</figcaption>
</figure>

<figure>
<img src="/assets/img/Screen Shot 2021-09-08 at 9.14.30 PM.png" alt="self-training">
<figcaption>Fig 5. Self-training structure</figcaption>
</figure>