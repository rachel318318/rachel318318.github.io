---
layout: post
title: âœï¸ U-stage | DL History & MLP
date: 2021-08-09 07:44 +0900
#modified: 2021-07-30 18:49 +0900
description: ë¨¸ì‹ ëŸ¬ë‹ ìˆ˜í•™ì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ê°œë…ë“¤ì„ í›‘ì–´ë³´ëŠ” ì‹œê°„
tag:
  - Boostcamp AI
  - U-stage
  - DL Basics
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. Introduction
---
##### What makes you a good deep learner?

* Implementation skills
* Math skills
* Knowing/reading a lot of recent papers

##### Composition

AI -> Machine Learning -> Deep Learning\
Mimic human intelligence through data-driven approach using neural networks

##### Key Components of Deep Learning

* **Data** that the model can learn from
* A **model** that transforms the data
* A **loss function** that quantifies the error of the model
* An **algorithm** to adjust the parameters to minimize the loss

# 2. DL History
---
##### 2012: AlexNet
ì´ë¯¸ì§€ ëŒ€íšŒì— ë”¥ëŸ¬ë‹ì„ ìµœì´ˆë¡œ ì ìš©í•˜ì—¬ 1ë“±í•¨\
ë§‰ì—°íˆ ìƒê°ë§Œ í•˜ë˜ ì¸ê°„ì˜ neural systemì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì„ ì‹¤ì œë¡œ êµ¬í˜„í•¨

##### 2013: DQN
ì•ŒíŒŒê³  ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ ë°©ë²•ìœ¼ë¡œ ìœ ëª…\
ë‹¹ì‹œ ìŠ¤íƒ€íŠ¸ì—…ì´ì—ˆë˜ DeepMindì—ì„œ êµ¬í˜„í•¨

##### 2014: Encoder/Decoder
NMT(Natural Machine Translation)ì„ êµ¬í˜„í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜\
êµ¬ê¸€ ë²ˆì—­ê¸°ê°€ ì“°ëŠ” ë°©ë²•

##### 2014: Adam Optimizer

##### 2015: Generative Adversarial Network (GAN)
Les Trois Brasseursë¼ëŠ” ìˆ ì§‘ì—ì„œ ìˆ  ë¨¹ë‹¤ ìˆ ì´ ë„ˆë¬´ ë§›ì´ ì—†ì–´ ìƒê°ì— ì ê¸´ ì™€ì¤‘ ë– ì˜¬ë¦° ì‹œìŠ¤í…œ

##### 2015: Residual Networks
ì „ì—ëŠ” ë ˆì´ì–´ë¥¼ ìŒ“ì•„ë„ ì–´ëŠ ìƒí•œì„ ì„ ë„˜ê¸°ë©´ ì„±ëŠ¥ì´ ë‚˜ë¹ ì¡Œì§€ë§Œ Residual Networksë¡œ ì¸í•˜ì—¬ ëª‡ê°œë¥¼ ìŒ“ì•„ë„ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ëª¨ìŠµì„ ë³´ì˜€ë‹¤. ì¦‰, ë”¥ëŸ¬ë‹ì„ ì œëŒ€ë¡œ êµ¬í˜„í•˜ê²Œ ë§Œë“  ìµœì´ˆì˜ ì‹œìŠ¤í…œ.

##### 2017: Transformer
êµ¬ê¸€ì—ì„œ íŒŒê²©ì ì¸ ì œëª©ìœ¼ë¡œ ë°œí‘œí•œ ë…¼ë¬¸ (Attention Is All You Need)

##### 2018: BERT (fine-tuned NLP models)

##### 2019: BIG Language Models
ì¼ëŸ° ë¨¸ìŠ¤í¬ì˜ OpenAIê°€ ê³µê°œí•œ ì´ˆê±°ëŒ€ AI 'GPT-3'ìœ¼ë¡œ ìœ ëª…

##### 2020: Self Supervised Learning
Labelì´ ì—†ëŠ” ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•

# 3. Neural Networks & Multi-layer Perceptron
---
ì‚¬ì‹¤ìƒ Deep learningì´ ëŒ€ì¶© ì‚¬ëŒì˜ ë‡Œë¥¼ ëª¨ë°©í–ˆë‹¤ëŠ” ì „ì œë³´ë‹¤ëŠ” ì™œ ì˜ë˜ëŠ”ì§€ ìˆ˜í•™ì ì¸ ë¶„ì„ì„ í†µí•´ì„œ \
ì—°êµ¬í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë‹¤.

> Neural networks are function approximators that stack affine transformations \
followed by nonlinear transformations.

##### MLP (Multi-Layer Perceptron)

<figure>
<img src="/assets/img/IMG_1235.jpg" alt="MLP">
<figcaption>Fig 4. Multi-Layer Perceptron</figcaption>
</figure>

$$W$$ = Weight \
$$b$$ = Bias \
$$\rho$$ = Nonlinear transform

ğŸˆ One way of interpreting a matrix is to regard it as a mapping between two vector spaces

\
ìì„¸í•œ ë‚´ìš©ì€ [DL Intro]({% post_url 2021-08-03-DLintro %}) ì°¸ê³ 