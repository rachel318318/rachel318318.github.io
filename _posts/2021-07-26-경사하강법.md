---
layout: post
title: 🌱 Pre-course | 경사하강법
date: 2021-07-26 16:44 +0900
#modified: 2021-07-30 18:49 +0900
description: 머신러닝 수학의 기초가 되는 개념들을 훑어보는 시간
tag:
  - Boostcamp AI
  - Pre-course
# image: /cara-memperbarui-fork-repository/repo.png
usemathjax: true
---

# 1. 경사하강법(순한맛)

### 미분

* 변수의 움직임에 따른 함수값의 변화를 측저하기 위한 도구
* 최적화에서 제일 많이 사용하는 기법

$$f'(x)=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}$$

* `sympy.diff`로 계산 가능

```py
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3), x)
```

* 미분은 함수 $$f$$의 주어진 점 $$(x,f(x))$$에서의 접선의 기울기를 구함
* 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있음

<figure>
<img src="/assets/img/IMG_1181.jpg" alt="접선의 기울기">
<figcaption>Fig 6. (x,f(x))에서의 접선의 기울기</figcaption>
</figure>

* 미분값을 더하면 경사상승법(gradient ascent), 미분값을 빼면 경사하강법(gradient descent)
* 각 함수의 극대값과 극소값의 위치를 구할 때 사용함

### 경사하강법: 알고리즘

```py
# gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while abs(grad) > eps:
  var = var - lr * grad
  grad = gradient(var)
```

🎈 컴퓨터로 미분이 정확히 0이 되는 것은 불가능하므로 `eps`보다 작을 때 종료하는 조건이 필요

🎈 `lr`은 학습률로서 미분을 통해 업데이트하는 속도를 조절함

### 변수가 벡터이면?

* 다변수 함수인 경우 편미분(partial differentiation)을 사용

$$\partial_{x_{i}}f(\textbf{x})=\lim_{h\rightarrow0}\frac{f(\textbf{x}+h\textbf{e}_{i})-f(\textbf{x})}{h}$$

* $$\textbf{e}_{i}$$는 $$i$$번째 값만 1이고 나머지는 0인 단위벡터
* 그레디언트(gradient) 벡터를 이용하여 경사하강/경사상승법에 적용

$$\nabla f=(\partial_{x_{1}}f,\partial_{x_{2}}f,\cdots,\partial_{x_{d}}f)$$

```py
import sympy as sym
from sympy.abc import x,y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)
# >> 2*x + 2*y - sin(x + 2*y)
```

<figure>
<img src="/assets/img/IMG_1182.jpg" alt="그레디언트 벡터">
<figcaption>Fig 7. 그레디언트 벡터</figcaption>
</figure>